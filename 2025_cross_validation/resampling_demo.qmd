---
title: "Resampling Demo"
author: "Kendra Wyant"
date: "`r lubridate::today()`"
format: 
  html: 
    embed-resources: true
    toc: true 
    toc_depth: 4
editor_options: 
  chunk_output_type: console
---   


## Introduction

In this demo, we will look at how to use cross-validation for model selection and evaluation. 


## Set up

### Load required packages 
```{r}
#| message: false

library(tidyverse) 
library(tidymodels)
library(mlbench) # for data set
library(kknn) # for knn modeling

theme_set(theme_classic())
```


### Set up parallel processing

Note you can type `cl` into your console to see how many cores your computer has. 
```{r}
cl <- parallel::makePSOCKcluster(parallel::detectCores(logical = FALSE))
doParallel::registerDoParallel(cl)
```


### Read in data

Lets load an existing R data set to work with
```{r}
data <- PimaIndiansDiabetes
```

```{r}
glimpse(data)

?PimaIndiansDiabetes
```

```{r}
levels(data$diabetes) # good that pos label is listed second!
```



## Evaluating single model with k-fold CV

### Split data

Splitting data in 10 folds, stratified on `diabetes`

*Why might it be important to stratify on the outcome variable?*

```{r}
data |> 
  count(diabetes) # 35% pos
```


```{r}
set.seed(102030)

splits_kfold <- data |> 
  vfold_cv(v = 10, repeats = 1, strata = "diabetes")

splits_kfold
```


### Build a recipe

Build a recipe that states `diabetes` is the outcome variable regressed on all predictors.  

Tidymodels has tons of other `step_()` functions that can be added to the recipe. For example, if we had any factor predictors, we could use `step_dummy()` to dummy code those variables within the recipe.

*By doing these steps within the recipe your ensuring that you are only manipulating the training data which prevents data leakage*

```{r}
rec <- recipe(diabetes ~ ., data = data) 
```


Fit a model with k-fold cross-validation. Use logistic regression as your statistical algorithm, `rec` as your recipe, and `accuracy` as your metric. 

```{r}
fits_log_reg <- logistic_reg() |> 
  set_engine("glm") |> 
  fit_resamples(preprocessor = rec,
                resamples = splits_kfold,
                metrics = metric_set(accuracy))
```

Examine performance estimates. Use the `collect_metrics()` function to make a table of the held-out performance estimates from the 10 folds
```{r}
metrics_log_reg <- collect_metrics(fits_log_reg, summarize = FALSE)

metrics_log_reg 
```

Plot a histogram of the performance estimates

*What do you notice?* 
```{r}
hist(metrics_log_reg$.estimate)
```

Print the average performance over folds with the `summarize = TRUE` argument.
```{r}
collect_metrics(fits_log_reg, summarize = TRUE)
```



## Using k-fold to tune hyperparameters and select best model configuration

### Split data

Lets work with a few more splits now!
```{r}
set.seed(102030)

splits_kfold_repeat <- data |> 
  vfold_cv(v = 10, repeats = 3, strata = "diabetes")

splits_kfold_repeat |> 
  print(n = Inf)
```

### Set up hyperparameter grid

Hyperparameters are parameters we have to specify to guide the machine learning algorithm. We are going to use a K-Nearest Neighbors ML algorithm. This has a hyperparameter `k` which stands for the number of neighbors it will use.

We often don't know what the best hyperparameter value will be beforehand so we consider a range of values using a tuning grid. We then can select the best hyperparameter value (i.e., best model configuration) based on our performance metric (in this exercise that would be accuracy).

Create a tibble with all values of the hyperparameter (`k`) we will consider. 
```{r}
hyper_grid <- expand.grid(neighbors = seq(2, 250, by = 5))
```

We are going to update our recipe so that all our numeric predictors are standardized
```{r}
?step_range

rec <- rec |> 
  step_range(all_numeric_predictors())
```

Tune the model with cross-validation. 
```{r}
fits_knn <- nearest_neighbor(neighbors = tune()) |> 
  set_engine("kknn") |> 
  set_mode("classification") |> 
  tune_grid(preprocessor = rec,
              resamples = splits_kfold_repeat,
              grid = hyper_grid,
              metrics = metric_set(accuracy))
```

Examine performance estimates across the held-out sets. When we use `summarize = TRUE` we get the average performance estimate across our 30 splits for each model configuration.
```{r }
collect_metrics(fits_knn, summarize = TRUE) |> 
  print(n = Inf)
```

Plot the average performance by hyperparameter value. 
```{r}
collect_metrics(fits_knn, summarize = TRUE) |> 
  ggplot() +
  geom_line(aes(x = neighbors, y = mean))
```


Print the performance of your best model configuration with the `show_best()` function.
```{r}
show_best(fits_knn, metric = "accuracy", n = 1)
```

lets look at histogram of 30 estimates for our best hyperparameter value
```{r}
best_knn <- collect_metrics(fits_knn, summarize = FALSE) |> 
  filter(neighbors == show_best(fits_knn, metric = "accuracy", n = 1)$neighbors)

hist(best_knn$.estimate)
```


