---
title: "Resampling"
author: "Kendra Wyant"
format: html
---

# Intro to Resampling and Cross-Validation
https://jjcurtin.github.io/book_iaml/l05_resampling.html 

## Problems with single sample


## Resampling

### Single train/test split
- Can also do a 3rd split validation

Cons:
- Holding out data for a test set (and possibly validation set) means using less data for training your model. Why is this a problem? 
- Evaluating model on single test set. Why could this be a problem?

### K-fold cross-validation

### Bootstrapping

### Nest Cross-validation
- Gold standard - selects the best model configuration on validation set and then evaluates on new held-out data.


## Lets fit a model 

### parallel processing

- what is it?
- CHTC


When using resampling, we often end up fitting many, many model configurations

This can be the same model configuration in many different training sets
Or many different model configurations in many different training sets (even more computationally demanding)

The fitting process for each of these configurations is independent for the others
The order that the configurations are fit doesnâ€™t matter either
When these two criteria are met, the processes can be run in parallel with an often big time savings

```{r}
cl <- parallel::makePSOCKcluster(parallel::detectCores(logical = FALSE))
doParallel::registerDoParallel(cl)
```

To see how many cores you have you can look at `cl` or type `parallel::detectCores(logical = FALSE)` in your console.

```{r}
cl
```